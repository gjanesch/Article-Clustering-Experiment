{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import newspaper\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_page_html(url):\n",
    "    \"\"\"Short function for grabbing a page's html.\"\"\"\n",
    "    return BeautifulSoup(requests.get(url).text, \"lxml\")\n",
    "\n",
    "def get_article_recommendations(next_article):\n",
    "    \"\"\"Extracts the recommended articles from a page.\"\"\"\n",
    "    article_html = get_page_html(next_article)\n",
    "    recommended_for_you = article_html.find(\"div\",attrs={'id':'article-rec'})\n",
    "    assert recommended_for_you is not None\n",
    "    if len(recommended_for_you) > 0:\n",
    "        articles = recommended_for_you.find_all(\"a\")\n",
    "        articles = [\"https://spectrum.ieee.org\" + a[\"href\"] for a in articles]\n",
    "    else:\n",
    "        articles = []\n",
    "    \n",
    "    is_not_sponsored = article_html.find(\"div\",attrs={\"class\":\"sponsors\"}) is None\n",
    "    \n",
    "    return [a for a in articles if not is_article_excluded(a)], is_not_sponsored\n",
    "\n",
    "# quick check to weed out obvious articles that don't fit the requirements\n",
    "def is_article_excluded(url):\n",
    "    \"\"\"\n",
    "    Check the URL for a few quick things that indicate that this\n",
    "    article doesn't fit what we're looking for.\n",
    "    \"\"\"\n",
    "    is_url_wrong = re.search(\"//spectrum\\.ieee\\.org/\", url) is None\n",
    "    is_whitepaper = re.search(\"/whitepaper/\", url) is not None\n",
    "    is_static = re.search(\"/static/\",url) is not None\n",
    "    is_media = re.search(\"/video/|/webinar/|/podcast/\",url) is not None\n",
    "    return is_media or is_whitepaper or is_static or is_url_wrong\n",
    "\n",
    "# figure out which category an article belongs to, for reference when we try clustering\n",
    "# the articles\n",
    "def get_article_type(url):\n",
    "    \"\"\"Determine the category of the article.\"\"\"\n",
    "    ieee_article_regex = \"^https://spectrum\\.ieee\\.org/(.*)/.*?$\"\n",
    "    article_type_string = re.match(ieee_article_regex, url)\n",
    "    if article_type_string is None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        article_types = article_type_string.group(1).split(\"/\")\n",
    "        article_categories = [atype for atype in article_types if atype in ARTICLE_CATEGORIES]\n",
    "        return article_categories[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ARTICLE_CATEGORIES = [\"aerospace\",\"at-work\",\"biomedical\",\"computing\",\"energy\",\"consumer-electronics\",\n",
    "                      \"geek-life\",\"green-tech\",\"tech-history\",\"robotics\",\"semiconductors\",\"telecom\",\"transportation\"]\n",
    "\n",
    "IEEE_ARTICLE_FILE = \"article_df.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the article file already exists, append to it; if not, start anew.\n",
    "# This was supposed to be if I wanted to collect a large number of articles\n",
    "# in the long term.\n",
    "if os.path.isfile(IEEE_ARTICLE_FILE):\n",
    "    article_df = pd.read_csv(IEEE_ARTICLE_FILE, sep = \"\\t\")\n",
    "    old_articles = article_df[\"URL\"].tolist()\n",
    "else:\n",
    "    article_df = pd.DataFrame({\"URL\":[],\"Category\":[],\"Article_Text\":[]})\n",
    "    article_df = article_df[[\"URL\",\"Category\",\"Article_Text\"]]\n",
    "    old_articles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grab the articles.\n",
    "ieee_spectrum = newspaper.build(\"https://spectrum.ieee.org/\", memoize_articles = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the article urls into a list, but filter out old and other\n",
    "# problematic ones.\n",
    "new_urls = [a.url for a in ieee_spectrum.articles]\n",
    "new_urls = [nu for nu in new_urls if nu not in old_articles]\n",
    "new_urls = [re.sub(\"://www\\.\", \"://\", nu) for nu in new_urls]\n",
    "new_urls = [nu for nu in new_urls if not is_article_excluded(nu)]\n",
    "len(new_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check each article found by newspaper for recommendations.  Anything new should be\n",
    "# added to the list of articles to check.  Continue until all new articles are\n",
    "# checked.\n",
    "\n",
    "seen_articles = []\n",
    "\n",
    "while len(new_urls) > 0:\n",
    "    print(f\"There are {len(new_urls)} unprocessed articles and {len(seen_articles)} new articles that have been stored.\")\n",
    "    all_articles = set(old_articles + seen_articles + new_urls)\n",
    "    \n",
    "    next_article = new_urls.pop(0)\n",
    "    next_article = re.sub(\"://www\\.\", \"://\", next_article)\n",
    "    print(\"Processing page \" + next_article)\n",
    "    try:\n",
    "        new_articles, article_is_good = get_article_recommendations(next_article)\n",
    "    except AssertionError:\n",
    "        print(\"***No recommendations in this article - moving on...***\")\n",
    "    else:\n",
    "        if article_is_good:\n",
    "            seen_articles.append(next_article)\n",
    "        if len(new_articles) > 0:\n",
    "            recommended_articles = [na for na in new_articles if na not in all_articles]\n",
    "        new_urls.extend(recommended_articles)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download articles and extract the text.\n",
    "# NOTE: article.download() intermittently failed for unclear reasons; the loop below\n",
    "# was structured to be easy to resume for when that happened.\n",
    "\n",
    "ArticleTuple = namedtuple(\"ArticleTuple\",[\"URL\",\"Category\",\"Article_Text\"])\n",
    "list_of_article_tuples = []\n",
    "\n",
    "for _ in tqdm.trange(len(seen_articles)):\n",
    "    article_url = seen_articles[0]\n",
    "    category = get_article_type(article_url)\n",
    "    article = newspaper.Article(article_url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article_tuple = ArticleTuple(URL = article_url, Category = category, Article_Text = article.text)\n",
    "    list_of_article_tuples.append(article_tuple)\n",
    "    seen_articles.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert to a dataframe and write to a csv.\n",
    "new_articles = pd.DataFrame(list_of_article_tuples)\n",
    "article_df = pd.concat([article_df, new_articles], axis = 0).reset_index(drop = True)\n",
    "article_df.to_csv(IEEE_ARTICLE_FILE, sep = \"\\t\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
